'''Calculate each user's average session time. A session is defined as the time difference between a page_load and page_exit. 
For simplicity, assume a user has only 1 session per day and if there are multiple of the same events on that day,
consider only the latest page_load and earliest page_exit, with an obvious restriction that load time event should happen before exit time event . 
Output the user_id and their average session time.'''

import pyspark.sql.functions as F
from pyspark.sql.window import Window

df = facebook_web_log.filter(F.col('action') == 'page_load').select('user_id', 'timestamp').alias('load') \
    .join(facebook_web_log.filter(F.col('action') == 'page_exit').select('user_id', 'timestamp').alias('exit'),
          on='user_id', how='left') \
    .filter(F.col('load.timestamp') < F.col('exit.timestamp'))
    

df = df.withColumn('date_load', F.date_trunc('day', F.col('load.timestamp')))
# Group by user_id and date_load, calculate max load and min exit timestamps per day
df = df.groupBy('user_id', 'date_load') \
    .agg(F.max('load.timestamp').alias('timestamp_load'), F.min('exit.timestamp').alias('timestamp_exit'))


df = df.withColumn('duration', F.col('timestamp_exit').cast('long') - F.col('timestamp_load').cast('long'))


df = df.filter(F.col('duration') > 0)

df = df.groupBy('user_id') \
    .agg(F.mean('duration').alias('avg_session_duration'))

# Convert the DataFrame to Pandas for final output
result = df.toPandas()

result

